---
title: "Math 3160 Notes"
layout: post
---

# 1: Combinatorics
<details>
<summary> Permutations (ordered): $\frac{n!}{(n-k)!}$ </summary>

3 unique awards, 10 students: $10 \times 9 \times 8 = \frac{10!}{(10-3)!}$
</details><br>

<details>
<summary> Combinations (unordered): $\binom{n}{k} = \frac{n!}{k!(n-k)!}$ </summary>

 3 exchangeable awards, 10 students: $\frac{10!}{(10-3)!}$ permuations, $3!$ ways to give 3 students awards, $\frac{10!}{3!(10-3)!} = \binom{10}{3}$
</details><br>

$\binom{n}{k} = \binom{n}{n-k}$, including $k$ = excluding $n-k$

Binomial theorem: $(x+y)^n = \sum_{k=1}^n \binom{n}{k} x^k y^{n-k}$

$\binom{n}{n_1, n_2, \dots, n_r} = \frac{n!}{n_1! n_2! \dots n_r!}$, dividing $n$ objects into groups of $n_i$


# 3: Independence
$E \perp \\!\\!\\! \perp F \\iff P(E \cup F) = P(E) \cdot P(F)$

<details>
<summary> $E \perp \!\!\! \perp F \implies E \perp \!\!\! \perp F^C$ </summary>

$P(E \cup F) = P(E) P(F)$<br>

$P(E) = P(E \cup F) + P(E \cup F^C)$<br>

$P(E) = P(E) P(F) + P(E \cup F^C)$<br>

$P(E \cup F^C) = P(E) (1-P(F)) = P(E) P(F^C)$
</details><br>

Bernoulli trials ($x$ successes out of $n$ independent trials): $\binom{n}{x} p^x (1-p)^{n-x}$


# 4: Conditional Probabilities
$P(E|F) = \frac{P(E \cup F)}{P(F)}$

<details>
<summary> $E \perp \!\!\! \perp F \iff P(E \| F) = P(E)$ </summary>

$P(E \cup F) = P(E) P(F) = P(E|F) P(F)$
</details><br>

**Bayes' Theorem**: $P(E\|F) = \frac{P(F\|E)P(E)}{P(F)}$


# 5: Discrete Random Variables
$\mathbb{E}[x] = \sum_x x p(x)$

$\mathbb{E}[ax + b] = a \mathbb{E}[x] + b$

$n$-th Moment: $\mathbb{E}[x^n]$

<details>
<summary>
$\text{Var}[x] = \mathbb{E}[(x - \mathbb{E}[x])^2] = \mathbb{E}[x^2] - \mathbb{E}[x]^2$
</summary><br>

$\text{Var}[x] = \mathbb{E}[x^2] - \mathbb{E}[2x \mathbb{E}[x]] + \mathbb{E}[x]^2 \\$

$ = \mathbb{E}[x^2] - 2 \mathbb{E}[x]^2 + \mathbb{E}[x]^2 \\$

$ = \mathbb{E}[x^2] - \mathbb{E}[x]^2 \\$
</details><br>


<details>
<summary>
$\text{Var}[ax + b] =  a^2 \text{Var}[x]$
</summary><br>

$\text{Var}[ax] = \mathbb{E}[a^2 x^2] - \mathbb{E}[ax]^2 \\$

$= a^2 (\mathbb{E}[x^2] - \mathbb{E}[x]^2) \\$

$= a^2 \text{Var}[x] \\ \\$


$\text{Var}[x+b] = \mathbb{E}[x^2 + 2xb + b^2] - \mathbb{E}[x + b]^2 \\$

$= \mathbb{E}[x^2] + 2b\mathbb{E}[x] + b^2 - (\mathbb{E}[x] + b)^2 \\$

$= \mathbb{E}[x^2] + 2b\mathbb{E}[x] + b^2 - (\mathbb{E}[x]^2 + 2b\mathbb{E}[x] + b^2) \\$

$= \mathbb{E}[x^2] - \mathbb{E}[x]^2 \\$

$=  \text{Var}[x]$
</details><br>

$\text{Std}(x) = \sqrt{\text{Var}(x)}$


Probability Mass Function (PMF): probability distribution over possible values of $X$

Cumulative Distribution Function (CDF): $F_X(x) = P(X \le x)$

$P(a \le x \le b) = F_X(b) - F_X(a)$


# 6: Discrete Distributions

### Bernoulli
$X \sim \text{Ber}(p)$

$P(X=1) = p \qquad P(X=0) = 1-p$

$\mathbb{E}[X] = 1(p) + 0(1-p) = p$

$\text{Var}[X] = \mathbb{E}[X^2] - \mathbb{E}[X]^2 = p - p^2 = p(1-p)$


### Binomial
$X \sim \text{Bin}(n, p)$

$P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}$

<details>
<summary>
$\mathbb{E}[X] = np \\$

$\text{Var}[X] = np(1-p)$
</summary><br>

$X = \sum_{i=1}^n Y_i \qquad Y_i \sim \text{Ber(p)} \\ \\$

$\mathbb{E}[X] = \mathbb{E}[\sum_{i=1}^n Y_i] = \sum_{i=1}^n \mathbb{E}[Y_i] = np \\ \\$

$\text{Var}[X] = \mathbb{E}[(\sum_{i=1}^n Y_i)^2] - \mathbb{E}[\sum_{i=1}^n Y_i]^2 \\$

$= \mathbb{E}[(\sum_{i=1}^n Y_i^2) + \sum_{i \ne j} Y_i Y_j] - n^2 p^2 \\$

$= np + (n^2 - n)p^2 - n^2 p^2 \\$

$= np - np^2 \\$

$= np(1-p)$
</details><br>


### Poisson
Event occurs **on avg** $\lambda$ times, $X$ is # of occurrences, then $X \sim \text{Pois}(\lambda)$

$P(X=i) = e^{-\lambda} \frac{\lambda^i}{i!}$

<details>
<summary>
$\sum_{i=0}^\infty e^{-\lambda} \frac{\lambda^i}{i!} = 1$
</summary>
<br>
$\sum_{i=0}^\infty e^{-\lambda} \frac{\lambda^i}{i!} = e^{-\lambda} \sum_{i=0}^\infty \frac{\lambda^i}{i!} \\$

$= e^{-\lambda} e^\lambda \qquad \text{by Taylor Series of } e^x \\$

$= 1$
</details><br>

$\mathbb{E}[X] = \sum_{i=0}^\infty i e^{-\lambda} \frac{\lambda^i}{i!} = \lambda e^{-\lambda} \sum_{i=1}^\infty \frac{\lambda^{i-1}}{(i-1)!} = \lambda$

For small $p$ and large $n$, $\text{Pois}(\lambda = np)$ estimates $\text{Bin}(n, p)$


### Geometric
$X$ is # (ind) trials to get 1st success, $X \sim \text{Geo}(p)$

$P(X=i) = (1-p)^{i-1} p$

$\mathbb{E}[X] = \frac{1}{p}$


### Negative Binomial
$X=n$, $r$-th success on $n$-th trial, $X \sim \text{NBin}(n, r)$

$P(X=n) = \binom{n-1}{r-1} p^r (1-p)^{n-r}$


### Hypergeometric
$X \sim \text{Hyp}(N, m, n)$

$P(X=i)$: sampling (without replacement) $i$ objects of interest, where there are $N$ total objects, $m$ objects of interest, and $n$ trials.

Ex: 20 eggs, 3 double yolk eggs, take 5 eggs. $X \sim \text{Hyp}(N=20, m=3, n=5)$. $P(2 \text{ double yolks}) = \frac{\binom{3}{2} \binom{17}{3}}{\binom{20}{5}}$

$P(X=i) = \frac{\binom{m}{i} \binom{N-m}{n-i}}{\binom{N}{m}}$



# 7: Continuous Distributions
Probability Density Function (PDF) $f(x)$: $P(a \le x \le b) = \int_a^b f(x) dx$

Cumulative Density Function (CDF): $F_X(y) = P(x \le y) = \int_{-\infty}^y f(x) dx$

$F_X'(y) = f(y)$

$\mathbb{E}[g(x)] = \int_{-\infty}^\infty g(x) f(x) dx$


### Uniform
$f_X(x) = \frac{1}{b-a}$

$\mathbb{E}[X] = \frac{b+a}{2}$
